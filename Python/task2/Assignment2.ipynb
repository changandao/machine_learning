{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-1-c4281150bd1c>, line 210)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-c4281150bd1c>\"\u001b[0;36m, line \u001b[0;32m210\u001b[0m\n\u001b[0;31m    print timeintv\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# assignment 2\n",
    "# Hang Xu, Sen Wang, Zhenglei Hu, Jianxiang Feng\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "#global parameters\n",
    "data_dir = '/home/luffy/Documents/Info_Retri_in_HD/assginments/2/yaleBfaces'\n",
    "print_vec = True\n",
    "\n",
    "class kpca_3nn(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def train(self, training_set_dir):\n",
    "        #inputs: training_set_dir: path of training set\n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        os.chdir(training_set_dir)\n",
    "        s0_list = glob.glob('*.png')\n",
    "        for f in s0_list:\n",
    "            img = Image.open(f).convert('L')\n",
    "            img = np.array(img)/255.\n",
    "            # get data of every image in training set and vectorize\n",
    "            self.X_train.append(img[:,:].ravel())\n",
    "            # get corresponding labels\n",
    "            self.y_train.append(int(os.path.split(f)[1].split('person')[-1][:2]))\n",
    "        #convert T and T_label into array\n",
    "        # T.shape: d*n, where d is dimension and n is number of samples\n",
    "        self.X_train = np.asarray(self.X_train).T\n",
    "        # T_label.shape: (n,)\n",
    "        self.y_train = np.asarray(self.y_train)\n",
    "    \n",
    "    def predict(self, X_test, k, y_test=None, start_from = 0):\n",
    "        # inputs: X_test: test data with shape d*n, y_test: label with shape(n,), k: number of PCs\n",
    "        # return: if y_test are given, return the misclassification rate\n",
    "        n_test = X_test.shape[1]\n",
    "        n_train = self.X_train.shape[1]\n",
    "        self.u = get_k_SigVec(self.X_train, k, start_from = start_from) # shape: d*k    \n",
    "        # extract k PCs from training set, which will be applied in test set\n",
    "        kpca_X_train= self.u.T.dot(self.X_train) # shape: k*n_train\n",
    "        # extract k PCs from test set\n",
    "        kpca_X_test = self.u.T.dot(X_test) # shape: k*n_test\n",
    "        \n",
    "        # compute distance matrix between training set and test set\n",
    "        dis_matrix = np.zeros((n_test, n_train))\n",
    "        sum_tr = np.sum(kpca_X_train ** 2, axis = 0)\n",
    "        sum_te = np.sum(kpca_X_test ** 2, axis = 0)\n",
    "        dis_matrix = np.sqrt(sum_tr[None, :] + sum_te[:, None] - 2*kpca_X_test.T.dot(kpca_X_train))\n",
    "        \n",
    "        # predict labels of test samples based on 3 nearest neighbour in dis_matrix\n",
    "        y_pre = np.zeros(n_test)\n",
    "        sorted_idx = np.argsort(dis_matrix,axis=1) # values are sorted in ascending order \n",
    "        for i in xrange(n_test):\n",
    "            closest_y = []\n",
    "            closest_y.extend(self.y_train[sorted_idx[i, :3]])\n",
    "            #print closest_y\n",
    "            count = np.zeros(11)\n",
    "            for j in closest_y:\n",
    "                count[j] += 1\n",
    "            y_pre[i] = np.argsort(count)[-1]\n",
    "        \n",
    "        # if test labels are given, compute the misclassification rate(error rate)\n",
    "        #print y_test.shape, len(y_pre)\n",
    "        if y_test != None:\n",
    "            error_rate = 1 - np.sum(y_test == y_pre,dtype=float)/n_test\n",
    "            return error_rate\n",
    "            \n",
    "\n",
    "def get_k_SigVec(X, k, start_from = 0):\n",
    "    #input param: X: raw data in vectorized form with shape d*n, k:number of the first singular vectors in U\n",
    "    #return: k PC vectors\n",
    "    #nomarlize the raw data\n",
    "    X_mean = X - np.mean(X, axis=1)[:,None]\n",
    "    X_std =  X/np.sqrt(np.sum(X_mean ** 2, axis=1))[:,None]\n",
    "    u, s, v = np.linalg.svd(X_std)\n",
    "    return u[:,start_from:k + start_from]\n",
    "\n",
    "start = time.clock()\n",
    "training_set_dir = data_dir + '/subset0'\n",
    "###(1)\n",
    "# extract k PC from raw data X\n",
    "k = 20\n",
    "X_train = []\n",
    "y_train = []\n",
    "os.chdir(training_set_dir)\n",
    "s0_list = glob.glob('*.png')\n",
    "for f in s0_list:\n",
    "    img = Image.open(f).convert('L')\n",
    "    img = np.array(img)/255.\n",
    "    # get data of every image in training set and vectorize\n",
    "    X_train.append(img[:,:].ravel())\n",
    "    # get corresponding labels\n",
    "    y_train.append(int(os.path.split(f)[1].split('person')[-1][:2]))\n",
    "#convert T and T_label into array\n",
    "# T.shape: d*n, where d is dimension and n is number of samples\n",
    "X_train = np.asarray(X_train).T\n",
    "u_k = get_k_SigVec(X_train, k)\n",
    "#print out the first three sigular vector\n",
    "if print_vec:\n",
    "    for i in range(3):\n",
    "        sig_vec = u_k[:,i].reshape((50,50))\n",
    "        plt.subplot(1,3,i+1)\n",
    "        plt.imshow(sig_vec)\n",
    "        \n",
    "        \n",
    "###(2)        \n",
    "# construct classifier\n",
    "kpca_3nn_classifier = kpca_3nn()\n",
    "kpca_3nn_classifier.train(training_set_dir)\n",
    "\n",
    "# get test data from subset1-4 and predict\n",
    "### start from 1th\n",
    "k_num = 20\n",
    "error_rate = np.zeros((4, k_num))\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(4):\n",
    "    test_set_dir = data_dir + '/subset' + str(i+1)\n",
    "    os.chdir(test_set_dir)\n",
    "    s_list = glob.glob('*.png')\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for f in s_list:\n",
    "        img = Image.open(f).convert('L')\n",
    "        img = np.array(img)/255.\n",
    "        # get data of every image in test set and vectorize\n",
    "        X_test.append(img[:,:].ravel())\n",
    "        # get corresponding labels\n",
    "        y_test.append(int(os.path.split(f)[1].split('person')[-1][:2]))\n",
    "    \n",
    "    X_test = np.array(X_test).T\n",
    "    y_test = np.array(y_test)\n",
    "    for k in range(k_num):\n",
    "        error_rate[i,k] = kpca_3nn_classifier.predict(X_test, k+1, y_test = y_test)\n",
    "    # plot the error rate of this subset\n",
    "    ax.plot(np.arange(k_num), error_rate[i], '*-',label=\"subset\"+str(i+1))\n",
    "    \n",
    "ax.set_xlabel('number of PCs')\n",
    "ax.set_ylabel('error_rate')\n",
    "ax.set_title('error_rate of subset1-4 starting from the 1st PC')\n",
    "ax.legend(loc=0); # upper left corner\n",
    "plt.show() \n",
    "\n",
    "### start from 3th\n",
    "k_num = 17\n",
    "error_rate = np.zeros((4, k_num))\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(4):\n",
    "    test_set_dir = data_dir + '/subset' + str(i+1)\n",
    "    os.chdir(test_set_dir)\n",
    "    s_list = glob.glob('*.png')\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for f in s_list:\n",
    "        img = Image.open(f).convert('L')\n",
    "        img = np.array(img)/255.\n",
    "        # get data of every image in test set and vectorize\n",
    "        X_test.append(img[:,:].ravel())\n",
    "        # get corresponding labels\n",
    "        y_test.append(int(os.path.split(f)[1].split('person')[-1][:2]))\n",
    "    \n",
    "    X_test = np.array(X_test).T\n",
    "    y_test = np.array(y_test)\n",
    "    for k in range(k_num):\n",
    "        error_rate[i,k] = kpca_3nn_classifier.predict(X_test, k+1, y_test = y_test, start_from = 3)\n",
    "    # plot the error rate of this subset\n",
    "    ax.plot(np.arange(k_num), error_rate[i], '*-',label=\"subset\"+str(i+1))\n",
    "    \n",
    "ax.set_xlabel('number of PCs')\n",
    "ax.set_ylabel('error_rate')\n",
    "ax.set_title('error_rate of subset1-4 starting from the 3th PC')\n",
    "ax.legend(loc=0); # upper left corner   \n",
    "plt.show()  \n",
    "\n",
    "### start from 5th\n",
    "k_num = 17\n",
    "error_rate = np.zeros((4, k_num))\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(4):\n",
    "    test_set_dir = data_dir + '/subset' + str(i+1)\n",
    "    os.chdir(test_set_dir)\n",
    "    s_list = glob.glob('*.png')\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for f in s_list:\n",
    "        img = Image.open(f).convert('L')\n",
    "        img = np.array(img)/255.\n",
    "        # get data of every image in test set and vectorize\n",
    "        X_test.append(img[:,:].ravel())\n",
    "        # get corresponding labels\n",
    "        y_test.append(int(os.path.split(f)[1].split('person')[-1][:2]))\n",
    "    \n",
    "    X_test = np.array(X_test).T\n",
    "    y_test = np.array(y_test)\n",
    "    for k in range(k_num):\n",
    "        error_rate[i,k] = kpca_3nn_classifier.predict(X_test, k+1, y_test = y_test, start_from = 5)\n",
    "    # plot the error rate of this subset\n",
    "    ax.plot(np.arange(k_num), error_rate[i], '*-',label=\"subset\"+str(i+1))\n",
    "    \n",
    "ax.set_xlabel('number of PCs')\n",
    "ax.set_ylabel('error_rate')\n",
    "ax.set_title('error_rate of subset1-4 starting from the 5th PC')\n",
    "ax.legend(loc=0) # upper left corner   \n",
    "plt.show()\n",
    "finish = time.clock()\n",
    "timeintv = finish - start\n",
    "print timeintv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reason for the difference between two recognition rate above\n",
    "As we know, the illuminate condition get worse and worse with the growth of number of subset. At subset4, even human cannot accurately recognize the faces. This is also expressed in the 3nn method with pca. In the top grahp, the error rate in subset1 decrease exponentially with the growth of number of principal components, while the error rate in the rest three subset decrease much less than subset0, especially in the subset3 and 4, there is nearly no decrease.\n",
    "\n",
    "In the second graph, after applying a little trick, that is to use the principle components from the 4th one, the method has much better performance in all subset than before. In subset1, the error rate decrease faster, and in subset2 the performance seems to catch up with that in subset1. In the last two worst illumination condition subset, there is also better performance.\n",
    "\n",
    "As far as I am concerned, there are several reasons for that:\n",
    "1. the first few principle components possess not only the most part of energy but also the most part of noise. Therefore removing the first few components can be treated as filtering noise.\n",
    "</br />  \n",
    "2. On the other hand, there are still much information stored in the components after the first few components, so the method can still work based on these information. This can be proved by the last graph above, when we choose principal components from 5th, the performance seems not to be better than before. The error rate in subset 1 decrease slower and performance in those in subset2 even get worse. That means that much more useful information have been filtered although much noise have also been filered."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}